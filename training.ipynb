{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Overview\n",
        "- Here we will be training a pretrained model. In this case the model is `bert-base-multilingual-uncased` from `Huggingface`.\n",
        "- The tokenizer we will be using is `BertTokenizer`\n",
        "- We will be using the `GoEmotions` dataset that was preprocessed earlier for training purposes.\n",
        "\n"
      ],
      "metadata": {
        "id": "1JQD0vHGWzXh"
      },
      "id": "1JQD0vHGWzXh"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "91ba9e90-02ca-48a4-8439-34947c375508",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "91ba9e90-02ca-48a4-8439-34947c375508",
        "outputId": "efc99872-e5e1-4c47-b6d5-4f39dbcfeaa5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torchviz in /usr/local/lib/python3.10/dist-packages (0.0.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from torchviz) (2.1.0+cu118)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.10/dist-packages (from torchviz) (0.20.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->torchviz) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->torchviz) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->torchviz) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->torchviz) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->torchviz) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->torchviz) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch->torchviz) (2.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->torchviz) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->torchviz) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn import metrics\n",
        "import transformers\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from transformers import BertTokenizer, BertModel, BertConfig\n",
        "from torch import cuda\n",
        "!pip install torchviz\n",
        "from torchviz import make_dot\n",
        "device = 'cuda:0' if cuda.is_available() else 'cpu'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive/\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "giAJgAByB4UG",
        "outputId": "832a72f7-074c-4be8-f108-423a4c6bb4ab"
      },
      "id": "giAJgAByB4UG",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "5171570a-51f9-4451-869f-a8b42993853a",
      "metadata": {
        "id": "5171570a-51f9-4451-869f-a8b42993853a"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, X_val = np.load(\"/content/drive/MyDrive/NLP/Datasets/goemotion_train_text_processed.npz\", allow_pickle=True), np.load(\"/content/drive/MyDrive/NLP/Datasets/goemotion_test_text_processed.npz\", allow_pickle=True), np.load(\"/content/drive/MyDrive/NLP/Datasets/goemotion_val_text_processed.npz\", allow_pickle=True)\n",
        "X_train, X_test, X_val = X_train.f.arr_0, X_test.f.arr_0, X_val.f.arr_0\n",
        "y_train, y_test, y_val = np.load(\"/content/drive/MyDrive/NLP/Datasets/goemotion_train_labels.npz\", allow_pickle=True), np.load(\"/content/drive/MyDrive/NLP/Datasets/goemotion_test_labels.npz\", allow_pickle=True), np.load(\"/content/drive/MyDrive/NLP/Datasets/goemotion_val_labels.npz\", allow_pickle=True)\n",
        "y_train, y_test, y_val = y_train.f.arr_0, y_test.f.arr_0, y_val.f.arr_0"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Defining parameters"
      ],
      "metadata": {
        "id": "3tkNy0CPaQ5u"
      },
      "id": "3tkNy0CPaQ5u"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "4836036d-786a-4d22-b831-0312d058089b",
      "metadata": {
        "id": "4836036d-786a-4d22-b831-0312d058089b"
      },
      "outputs": [],
      "source": [
        "MAX_LEN = 50\n",
        "BATCH_SIZE = 32\n",
        "EPOCHS = 4\n",
        "LEARNING_RATE = 1e-05\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-uncased')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Defining Custom Dataset using `Dataset` class\n",
        "Here in constructor\n",
        "- X : The list of sentences\n",
        "- label : The class labeled in the dataset against each sentence\n",
        "- tokenizer : The tokenizer to be used in embedding the words\n",
        "- max_len : Length of the sequence\n",
        "\n",
        "In `__getitem__(self, index)` method\n",
        "- the input text string returs `input_ids`, `attention_mask`, `token_type_ids` as input arguments for the `model`."
      ],
      "metadata": {
        "id": "-gD3iNl1alao"
      },
      "id": "-gD3iNl1alao"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "a69fd083-c2df-4a01-978f-cb7aee2b289c",
      "metadata": {
        "id": "a69fd083-c2df-4a01-978f-cb7aee2b289c"
      },
      "outputs": [],
      "source": [
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, X, label, tokenizer, max_len):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.text = X\n",
        "        a = label\n",
        "        labels = torch.zeros((a.size, a.max() + 1))\n",
        "        labels[np.arange(a.size), a] = 1\n",
        "        self.targets = labels.type(torch.float).to(device)\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.text)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        text = str(self.text[index])\n",
        "        text = \" \".join(text.split())\n",
        "\n",
        "        inputs = self.tokenizer.encode_plus(\n",
        "            text,\n",
        "            None,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_len,\n",
        "            padding='max_length',\n",
        "            return_token_type_ids=True\n",
        "        )\n",
        "        ids = inputs['input_ids']\n",
        "        mask = inputs['attention_mask']\n",
        "        token_type_ids = inputs[\"token_type_ids\"]\n",
        "\n",
        "\n",
        "        return {\n",
        "            'ids': torch.tensor(ids, dtype=torch.long),\n",
        "            'mask': torch.tensor(mask, dtype=torch.long),\n",
        "            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\n",
        "            'targets': self.targets[index].clone().detach().requires_grad_(True)\n",
        "        }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "a43ecf1c-5c87-4f79-abb2-ad21b60c2a6e",
      "metadata": {
        "id": "a43ecf1c-5c87-4f79-abb2-ad21b60c2a6e"
      },
      "outputs": [],
      "source": [
        "training_set = CustomDataset(X_train, y_train, tokenizer, MAX_LEN)\n",
        "testing_set = CustomDataset(X_test, y_test, tokenizer, MAX_LEN)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Perparing Datasets for training in batches of size 4\n"
      ],
      "metadata": {
        "id": "EDD-UDn4cXis"
      },
      "id": "EDD-UDn4cXis"
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "3d672c2b-ef51-4076-94be-9aecaaec10be",
      "metadata": {
        "id": "3d672c2b-ef51-4076-94be-9aecaaec10be"
      },
      "outputs": [],
      "source": [
        "train_params = {'batch_size': BATCH_SIZE,\n",
        "                'shuffle': True,\n",
        "                'num_workers': 0\n",
        "                }\n",
        "\n",
        "test_params = {'batch_size': 1,\n",
        "                'shuffle': True,\n",
        "                'num_workers': 0\n",
        "                }\n",
        "\n",
        "training_loader = DataLoader(training_set, **train_params)\n",
        "testing_loader = DataLoader(testing_set, **test_params)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Defining and Initiating the `model`\n",
        "- Using weights from `HuggingFace` `BERT` multilingual uncased version\n",
        "- On the original output from `BERT`, it has a `logits` layer with `768` features\n",
        "- To fine tune it a `Dropout` layer and a `Linear` layer has been added sequentially\n",
        "- Final layer has `28` outputs\n",
        "\n"
      ],
      "metadata": {
        "id": "-BUeDxoGcmOW"
      },
      "id": "-BUeDxoGcmOW"
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "a43a1b15-7f71-41ce-9b0e-b26dcb3341b8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a43a1b15-7f71-41ce-9b0e-b26dcb3341b8",
        "outputId": "f5b0ac31-d409-40fe-94b6-5ba6b1a17473"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BERTClass(\n",
              "  (l1): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(105879, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0-11): 12 x BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (l2): Dropout(p=0.3, inplace=False)\n",
              "  (l3): Linear(in_features=768, out_features=28, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "class BERTClass(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(BERTClass, self).__init__()\n",
        "        self.l1 = transformers.BertModel.from_pretrained('bert-base-multilingual-uncased')\n",
        "        self.l2 = torch.nn.Dropout(0.3)\n",
        "        self.l3 = torch.nn.Linear(768, 28)\n",
        "\n",
        "    def forward(self, ids, mask, token_type_ids):\n",
        "        _, output_1= self.l1(ids, attention_mask = mask, token_type_ids = token_type_ids, return_dict=False)\n",
        "        output_2 = self.l2(output_1)\n",
        "        output = self.l3(output_2)\n",
        "        return output\n",
        "\n",
        "model = BERTClass()\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Saving a view of the `model` to be finetuned"
      ],
      "metadata": {
        "id": "5lRJ5h_AeXo2"
      },
      "id": "5lRJ5h_AeXo2"
    },
    {
      "cell_type": "code",
      "source": [
        "# Create dummy input tensors\n",
        "dummy_ids = torch.randint(0, 100, (1, 128)).to(device)\n",
        "dummy_mask = torch.randint(0, 2, (1, 128)).to(device)\n",
        "dummy_token_type_ids = torch.randint(0, 2, (1, 128)).to(device)\n",
        "\n",
        "# Generate a visualization of the model's computation graph\n",
        "dot = make_dot(model(dummy_ids, dummy_mask, dummy_token_type_ids),\n",
        "               params=dict(model.named_parameters()))\n",
        "\n",
        "# Save the graph to a file\n",
        "dot.render(\"bert_model_graph\")\n",
        "\n",
        "# Display the graph\n",
        "dot.view()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "C6rcTjzBRndd",
        "outputId": "80d2678f-8757-4d02-d93f-f24022c6a255"
      },
      "id": "C6rcTjzBRndd",
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'bert_model_graph.pdf'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Defining `optimizer` and `loss`"
      ],
      "metadata": {
        "id": "6_NqQZoPemGX"
      },
      "id": "6_NqQZoPemGX"
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "41d1a7ff-b206-4c94-b87a-7a01f7566ef5",
      "metadata": {
        "id": "41d1a7ff-b206-4c94-b87a-7a01f7566ef5"
      },
      "outputs": [],
      "source": [
        "def loss_fn(outputs, targets):\n",
        "    return torch.nn.CrossEntropyLoss()(outputs, targets)\n",
        "\n",
        "optimizer = torch.optim.Adam(params =  model.parameters(), lr=LEARNING_RATE)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. Training the `model` for `4` epochs with batch size `4`"
      ],
      "metadata": {
        "id": "DU7aYToWfi0T"
      },
      "id": "DU7aYToWfi0T"
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "f878db74-f983-4919-9d40-4022e1f16a3b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f878db74-f983-4919-9d40-4022e1f16a3b",
        "outputId": "53fbf784-0092-4dee-e532-a6bf367a0952"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0, Loss:  3.3253016471862793\n",
            "Validation Loss: 3.2862547790013514\n",
            "Epoch: 1, Loss:  2.938786745071411\n",
            "Validation Loss: 3.008619713480134\n",
            "Epoch: 2, Loss:  1.847422480583191\n",
            "Validation Loss: 1.6520635094888327\n",
            "Epoch: 3, Loss:  1.2256757020950317\n",
            "Validation Loss: 1.5537537498485763\n"
          ]
        }
      ],
      "source": [
        "def evaluate(model, data_loader, loss_fn, device):\n",
        "    model.eval()\n",
        "    total_loss = 0.0\n",
        "    total_samples = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for data in data_loader:\n",
        "            ids = data['ids'].to(device, dtype = torch.long)\n",
        "            mask = data['mask'].to(device, dtype = torch.long)\n",
        "            token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n",
        "            targets = data['targets'].to(device, dtype = torch.float)\n",
        "            outputs = model(ids, mask, token_type_ids)\n",
        "            loss = loss_fn(outputs, targets)\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            total_samples += targets.size(0)\n",
        "\n",
        "    average_loss = total_loss / total_samples\n",
        "    print(f'Validation Loss: {average_loss}')\n",
        "\n",
        "\n",
        "def train(epoch):\n",
        "    model.train()\n",
        "    try:\n",
        "      for _, data in enumerate(training_loader, 0):\n",
        "          ids = data['ids'].to(device, dtype = torch.long)\n",
        "          mask = data['mask'].to(device, dtype = torch.long)\n",
        "          token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n",
        "          targets = data['targets'].to(device, dtype = torch.float)\n",
        "\n",
        "          outputs = model(ids, mask, token_type_ids)\n",
        "\n",
        "          optimizer.zero_grad()\n",
        "          loss = loss_fn(outputs, targets)\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "          if _%1000==0:\n",
        "              print(f'Epoch: {epoch}, Loss:  {loss.item()}')\n",
        "              if testing_loader is not None:\n",
        "                evaluate(model, testing_loader, loss_fn, device)\n",
        "    except Exception as e:\n",
        "      pass\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    train(epoch)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8. Saving the model states for inference"
      ],
      "metadata": {
        "id": "oWwJ0yb4fvbY"
      },
      "id": "oWwJ0yb4fvbY"
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "efdfc45a-505e-4b44-a329-9c302a543146",
      "metadata": {
        "id": "efdfc45a-505e-4b44-a329-9c302a543146"
      },
      "outputs": [],
      "source": [
        "torch.save(model.state_dict(), \"/content/drive/MyDrive/NLP/model.pt\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8dCqFbC3YO6e"
      },
      "id": "8dCqFbC3YO6e",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}