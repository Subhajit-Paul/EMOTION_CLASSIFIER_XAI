{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Overview\n",
        " - The GoEmotions dataset provides a large, manually annotated, dataset for fine-grained emotion prediction.\n",
        " - The dataset is curated by Google high coverage of the emotions expressed in Reddit comments.\n",
        " - The Dataset contains 28 different Emotions\n",
        "\n",
        "### Find more at this [Blog](https://blog.research.google/2021/10/goemotions-dataset-for-fine-grained.html)"
      ],
      "metadata": {
        "id": "dgX45sjSJ8x5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k_vmh0YjgyfT",
        "outputId": "536b23bd-6646-40b4-b11f-d9e5b9a51fbe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive/\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install emoji\n",
        "!pip install cleantext\n",
        "!pip install contractions"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QXfKCOBX6tlB",
        "outputId": "3eabd345-3577-4a16-a275-51c71b95483a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: emoji in /usr/local/lib/python3.10/dist-packages (2.8.0)\n",
            "Requirement already satisfied: cleantext in /usr/local/lib/python3.10/dist-packages (1.1.4)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from cleantext) (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->cleantext) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->cleantext) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->cleantext) (2023.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk->cleantext) (4.66.1)\n",
            "Collecting contractions\n",
            "  Downloading contractions-0.1.73-py2.py3-none-any.whl (8.7 kB)\n",
            "Collecting textsearch>=0.0.21 (from contractions)\n",
            "  Downloading textsearch-0.0.24-py2.py3-none-any.whl (7.6 kB)\n",
            "Collecting anyascii (from textsearch>=0.0.21->contractions)\n",
            "  Downloading anyascii-0.3.2-py3-none-any.whl (289 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m289.9/289.9 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pyahocorasick (from textsearch>=0.0.21->contractions)\n",
            "  Downloading pyahocorasick-2.0.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (110 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m110.8/110.8 kB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pyahocorasick, anyascii, textsearch, contractions\n",
            "Successfully installed anyascii-0.3.2 contractions-0.1.73 pyahocorasick-2.0.0 textsearch-0.0.24\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Loading important Libraries"
      ],
      "metadata": {
        "id": "nplbHmnvLPro"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "id1RFuWbzFGY"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn import metrics\n",
        "from bs4 import BeautifulSoup\n",
        "import os\n",
        "import nltk\n",
        "import re\n",
        "import string\n",
        "import json\n",
        "from cleantext import clean\n",
        "import emoji\n",
        "import string\n",
        "import seaborn as sns\n",
        "from tqdm import tqdm\n",
        "import contractions"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Loading the Data\n",
        "\n",
        "- Use the Hugging face `Datasets` library to get it in 3 splits ( `train`, `test`, `validation` )\n",
        "- Use `kaggle` accounts to get the data from [here](https://www.kaggle.com/datasets/shivamb/go-emotions-google-emotions-dataset)\n",
        "\n",
        "The data can be found [here](https://huggingface.co/datasets/go_emotions)"
      ],
      "metadata": {
        "id": "vxgDeuWnLfpJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train = pd.read_csv(\"/content/drive/MyDrive/NLP/Datasets/go_train.csv\").drop(['id'], axis = 1)\n",
        "test  = pd.read_csv(\"/content/drive/MyDrive/NLP/Datasets/go_test.csv\").drop(['id'], axis = 1)\n",
        "val  = pd.read_csv(\"/content/drive/MyDrive/NLP/Datasets/go_val.csv\").drop(['id'], axis = 1)"
      ],
      "metadata": {
        "id": "Px2kNMsM65Oq"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import operator\n",
        "def check_coverage(vocab,embeddings_index):\n",
        "    a = {}\n",
        "    oov = {}\n",
        "    k = 0\n",
        "    i = 0\n",
        "    for word in tqdm(vocab):\n",
        "        try:\n",
        "            a[word] = embeddings_index[word]\n",
        "            k += vocab[word]\n",
        "        except:\n",
        "\n",
        "            oov[word] = vocab[word]\n",
        "            i += vocab[word]\n",
        "            pass\n",
        "\n",
        "    print('Found embeddings for {:.2%} of vocab'.format(len(a) / len(vocab)))\n",
        "    print('Found embeddings for  {:.2%} of all text'.format(k / (k + i)))\n",
        "    sorted_x = sorted(oov.items(), key=operator.itemgetter(1))[::-1]\n",
        "    return sorted_x\n",
        "\n",
        "def build_vocab(sentences, verbose =  True):\n",
        "    vocab = {}\n",
        "    for sentence in tqdm(sentences, disable = (not verbose)):\n",
        "        for word in sentence:\n",
        "            try:\n",
        "                vocab[word] += 1\n",
        "            except KeyError:\n",
        "                vocab[word] = 1\n",
        "    return vocab\n",
        "\n",
        "def get_coefs(word, *arr):\n",
        "    return word, np.asarray(arr, dtype='float32')\n",
        "\n",
        "\n",
        "def load_embeddings(path):\n",
        "    with open(path, encoding=\"utf-8\") as f:\n",
        "        return dict(get_coefs(*line.strip().split(' ')) for line in f)\n",
        "\n",
        "\n",
        "def build_matrix(word_index, path):\n",
        "    embedding_index = load_embeddings(path)\n",
        "    embedding_matrix = np.zeros((len(word_index) + 1, 300))\n",
        "    for word, i in word_index.items():\n",
        "        try:\n",
        "            embedding_matrix[i] = embedding_index[word]\n",
        "        except KeyError:\n",
        "            pass\n",
        "    return embedding_matrix\n",
        "\n",
        "def remove_emojis(text: str) -> str:\n",
        "    return ''.join(c for c in text if c not in emoji.EMOJI_DATA)\n",
        "\n",
        "glove_embeddings = load_embeddings(r\"/content/drive/MyDrive/NLP/resources/glove.840B.300d.txt\")\n",
        "\n",
        "white_list = string.ascii_letters + string.digits + ' '\n",
        "white_list += \"'-\"\n",
        "glove_chars = ''.join([c for c in tqdm(glove_embeddings) if len(c) == 1])\n",
        "glove_symbols = ''.join([c for c in glove_chars if not c in white_list])\n",
        "review_chars = build_vocab(list(train.text))\n",
        "review_symbols = ''.join([c for c in review_chars if not c in white_list])\n",
        "symbols_to_delete = ''.join([c for c in review_symbols if not c in glove_symbols])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qIJ_bezq7Sdx",
        "outputId": "caaa288a-8c5e-466d-da33-4fe98e9f8dec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2196007/2196007 [00:00<00:00, 3191111.09it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 43410/43410 [00:00<00:00, 127902.25it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = build_vocab(list(train.text.apply(lambda x:x.split())))\n",
        "oov = check_coverage(vocab,glove_embeddings)\n",
        "oov[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dumxprGB7aCs",
        "outputId": "66142cde-352b-44d0-f98f-ec6a053def9b"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 43410/43410 [00:00<00:00, 173318.51it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 56729/56729 [00:00<00:00, 478353.30it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found embeddings for 52.68% of vocab\n",
            "Found embeddings for  88.28% of all text\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('[NAME]', 5743),\n",
              " ('I‚Äôm', 934),\n",
              " ('[NAME].', 786),\n",
              " (\"That's\", 587),\n",
              " ('don‚Äôt', 563),\n",
              " ('[NAME],', 510),\n",
              " ('it‚Äôs', 476),\n",
              " ('It‚Äôs', 339),\n",
              " ('That‚Äôs', 335),\n",
              " (\"isn't\", 283)]"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "punc = '!?\"#$%&\\'()*+,-./:;<=>@[\\\\]^_`{|}~'\n",
        "def preprocessing1(cleantext):\n",
        "    cleantext = cleantext.lower()\n",
        "    cleantext = cleantext.replace(\"[NAME]\", \"X\").replace(\"[name]\", \"X\").replace(\".\", \"\").replace(\",\", \"\").replace('\"', \"\").replace(\"!!\", \"!\").replace(\"‚Äú\", \"\").replace(\"‚Äù\", \"\").replace(\"remindme\", \"remind me\").replace(\"altright\", \"alright\")\n",
        "    cleantext = contractions.fix(cleantext)\n",
        "    cleantext = \"\".join([ cleantext[i] for i in range(len(cleantext)) if cleantext[i] not in symbols_to_delete])\n",
        "    clean_text = remove_emojis(cleantext)\n",
        "    cleantext = cleantext.translate(str.maketrans(\"\", \"\", punc))\n",
        "    return cleantext.split()\n",
        "\n",
        "def preprocessing2(cleantext):\n",
        "    cleantext = cleantext.lower()\n",
        "    cleantext = cleantext.replace(\"[NAME]\", \"X\").replace(\"[name]\", \"X\").replace(\".\", \"\").replace(\",\", \"\").replace('\"', \"\").replace(\"!!\", \"!\").replace(\"‚Äú\", \"\").replace(\"‚Äù\", \"\").replace(\"remindme\", \"remind me\").replace(\"altright\", \"alright\")\n",
        "    cleantext = contractions.fix(cleantext)\n",
        "    cleantext = \"\".join([ cleantext[i] for i in range(len(cleantext)) if cleantext[i] not in symbols_to_delete])\n",
        "    clean_text = remove_emojis(cleantext)\n",
        "    cleantext = cleantext.translate(str.maketrans(\"\", \"\", punc))\n",
        "    return cleantext\n",
        "\n",
        "vocab = build_vocab(list(train.text.apply(preprocessing1)))\n",
        "oov = check_coverage(vocab,glove_embeddings)\n",
        "oov[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DseD7I4h8USV",
        "outputId": "ab488b71-60e3-47a6-d910-dc102587c6c5"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 43410/43410 [00:00<00:00, 221100.26it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 28657/28657 [00:00<00:00, 479130.72it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found embeddings for 85.23% of vocab\n",
            "Found embeddings for  99.16% of all text\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('incels', 21),\n",
              " ('incel', 21),\n",
              " ('brexit', 19),\n",
              " ('fortnite', 19),\n",
              " ('ü§î', 19),\n",
              " ('people‚Äôs', 15),\n",
              " ('altright', 14),\n",
              " ('shitposting', 11),\n",
              " ('üòÅ', 11),\n",
              " ('Õ°¬∞', 10)]"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train.text = train.text.apply(preprocessing2)"
      ],
      "metadata": {
        "id": "bAULtPUK85Gs"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = build_vocab(list(test.text.apply(lambda x:x.split())))\n",
        "oov = check_coverage(vocab,glove_embeddings)\n",
        "oov[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aiWOmRLvBQ0G",
        "outputId": "c633d319-09d5-492e-8c7c-2754749f5135"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5427/5427 [00:00<00:00, 151773.19it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 14019/14019 [00:00<00:00, 427978.37it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found embeddings for 67.32% of vocab\n",
            "Found embeddings for  88.29% of all text\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('[NAME]', 745),\n",
              " ('I‚Äôm', 122),\n",
              " ('[NAME].', 91),\n",
              " ('it‚Äôs', 64),\n",
              " (\"That's\", 62),\n",
              " ('don‚Äôt', 62),\n",
              " ('[NAME],', 57),\n",
              " ('It‚Äôs', 55),\n",
              " (\"he's\", 40),\n",
              " ('you!', 39)]"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "white_list = string.ascii_letters + string.digits + ' '\n",
        "white_list += \"'-\"\n",
        "glove_chars = ''.join([c for c in tqdm(glove_embeddings) if len(c) == 1])\n",
        "glove_symbols = ''.join([c for c in glove_chars if not c in white_list])\n",
        "review_chars = build_vocab(list(test.text))\n",
        "review_symbols = ''.join([c for c in review_chars if not c in white_list])\n",
        "symbols_to_delete = ''.join([c for c in review_symbols if not c in glove_symbols])\n",
        "def preprocessing1(text):\n",
        "    cleantext = text.lower()\n",
        "    x = cleantext.replace(\"[NAME]\", \"X\").replace(\"[name]\", \"X\").replace(\".\", \"\").replace(\",\", \"\").replace('\"', \"\").replace(\"!!\", \"!\").replace(\"‚Äú\", \"\").replace(\"‚Äù\", \"\").replace(\"remindme\", \"remind me\").replace(\"altright\", \"alright\")\n",
        "    cleantext = \"\".join([ x[i] for i in range(len(x)) if x[i] not in symbols_to_delete])\n",
        "    cleantext = contractions.fix(cleantext)\n",
        "    cleantext = cleantext.translate(str.maketrans(\"\", \"\", punc))\n",
        "    return cleantext.split()\n",
        "\n",
        "def preprocessing2(text):\n",
        "    cleantext = text.lower()\n",
        "    x = cleantext.replace(\"[NAME]\", \"X\").replace(\"[name]\", \"X\").replace(\".\", \"\").replace(\",\", \"\").replace('\"', \"\").replace(\"!!\", \"!\").replace(\"‚Äú\", \"\").replace(\"‚Äù\", \"\").replace(\"remindme\", \"remind me\").replace(\"altright\", \"alright\")\n",
        "    cleantext = \"\".join([ x[i] for i in range(len(x)) if x[i] not in symbols_to_delete])\n",
        "    cleantext = contractions.fix(cleantext)\n",
        "    cleantext = cleantext.translate(str.maketrans(\"\", \"\", punc))\n",
        "    return cleantext\n",
        "\n",
        "vocab = build_vocab(list(test.text.apply(preprocessing1)))\n",
        "oov = check_coverage(vocab,glove_embeddings)\n",
        "oov[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sSe05JVR-UWy",
        "outputId": "32e12cfd-95a0-42ed-ec4c-57adff379155"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2196007/2196007 [00:00<00:00, 3367473.75it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5427/5427 [00:00<00:00, 124359.36it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5427/5427 [00:00<00:00, 171444.30it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8706/8706 [00:00<00:00, 320959.92it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found embeddings for 94.02% of vocab\n",
            "Found embeddings for  99.23% of all text\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('brexit', 7),\n",
              " ('incel', 4),\n",
              " ('rfunny', 3),\n",
              " ('fortnite', 3),\n",
              " ('people‚Äôs', 3),\n",
              " ('incels', 3),\n",
              " ('lightspamming', 2),\n",
              " ('hbomb', 2),\n",
              " ('pwbpd', 2),\n",
              " ('citycounty', 2)]"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test.text = test.text.apply(preprocessing2)"
      ],
      "metadata": {
        "id": "eF9ijOe4-a0B"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = build_vocab(list(val.text.apply(lambda x:x.split())))\n",
        "oov = check_coverage(vocab,glove_embeddings)\n",
        "oov[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ctIia83pHK3z",
        "outputId": "44e0e771-d5e9-477f-ae93-c2775e3d75fa"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5426/5426 [00:00<00:00, 134971.14it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 14240/14240 [00:00<00:00, 389039.43it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found embeddings for 66.91% of vocab\n",
            "Found embeddings for  88.31% of all text\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('[NAME]', 711),\n",
              " ('I‚Äôm', 119),\n",
              " ('[NAME].', 96),\n",
              " (\"That's\", 76),\n",
              " ('it‚Äôs', 65),\n",
              " ('[NAME],', 65),\n",
              " ('It‚Äôs', 56),\n",
              " ('don‚Äôt', 56),\n",
              " ('That‚Äôs', 44),\n",
              " (\"You're\", 43)]"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "white_list = string.ascii_letters + string.digits + ' '\n",
        "white_list += \"'-\"\n",
        "glove_chars = ''.join([c for c in tqdm(glove_embeddings) if len(c) == 1])\n",
        "glove_symbols = ''.join([c for c in glove_chars if not c in white_list])\n",
        "review_chars = build_vocab(list(val.text))\n",
        "review_symbols = ''.join([c for c in review_chars if not c in white_list])\n",
        "symbols_to_delete = ''.join([c for c in review_symbols if not c in glove_symbols])\n",
        "def preprocessing1(text):\n",
        "    cleantext = text.lower()\n",
        "    x = cleantext.replace(\"[NAME]\", \"X\").replace(\"[name]\", \"X\").replace(\".\", \"\").replace(\",\", \"\").replace('\"', \"\").replace(\"!!\", \"!\").replace(\"‚Äú\", \"\").replace(\"‚Äù\", \"\").replace(\"remindme\", \"remind me\").replace(\"altright\", \"alright\")\n",
        "    cleantext = \"\".join([ x[i] for i in range(len(x)) if x[i] not in symbols_to_delete])\n",
        "    cleantext = contractions.fix(cleantext)\n",
        "    cleantext = cleantext.translate(str.maketrans(\"\", \"\", punc))\n",
        "    return cleantext.split()\n",
        "\n",
        "def preprocessing2(text):\n",
        "    cleantext = text.lower()\n",
        "    x = cleantext.replace(\"[NAME]\", \"X\").replace(\"[name]\", \"X\").replace(\".\", \"\").replace(\",\", \"\").replace('\"', \"\").replace(\"!!\", \"!\").replace(\"‚Äú\", \"\").replace(\"‚Äù\", \"\").replace(\"remindme\", \"remind me\").replace(\"altright\", \"alright\")\n",
        "    cleantext = \"\".join([ x[i] for i in range(len(x)) if x[i] not in symbols_to_delete])\n",
        "    cleantext = contractions.fix(cleantext)\n",
        "    cleantext = cleantext.translate(str.maketrans(\"\", \"\", punc))\n",
        "    return cleantext\n",
        "\n",
        "vocab = build_vocab(list(val.text.apply(preprocessing1)))\n",
        "oov = check_coverage(vocab,glove_embeddings)\n",
        "oov[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q_5Ku3_SBhVg",
        "outputId": "644e116d-99e6-400e-b349-29a175688d35"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2196007/2196007 [00:01<00:00, 1279031.42it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5426/5426 [00:00<00:00, 55300.19it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5426/5426 [00:00<00:00, 151405.02it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8890/8890 [00:00<00:00, 311829.82it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found embeddings for 93.98% of vocab\n",
            "Found embeddings for  99.22% of all text\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('chiiiiika', 4),\n",
              " ('90df', 3),\n",
              " ('shitpost', 3),\n",
              " ('acopia', 2),\n",
              " ('laop', 2),\n",
              " ('tlj', 2),\n",
              " ('one‚Äôs', 2),\n",
              " ('juul', 2),\n",
              " ('incel', 2),\n",
              " ('thibs', 2)]"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "val.text = val.text.apply(preprocessing2)"
      ],
      "metadata": {
        "id": "BAyRyIXCHVGI"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def labelPreprocess(text):\n",
        "    text = text.replace(\"[ \", \"\").replace(\"[\", \"\").replace(\"]\", \"\").split(\" \")[0]\n",
        "    return int(text)"
      ],
      "metadata": {
        "id": "Q-KcCArnHcZc"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train.labels = train.labels.apply(labelPreprocess)\n",
        "test.labels = test.labels.apply(labelPreprocess)\n",
        "val.labels = val.labels.apply(labelPreprocess)"
      ],
      "metadata": {
        "id": "aPBiZ6GvHmrD"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.savez_compressed(\"/content/drive/MyDrive/NLP/Datasets/goemotion_test_text_processed\", test.text.to_numpy())\n",
        "np.savez_compressed(\"/content/drive/MyDrive/NLP/Datasets/goemotion_train_text_processed\", train.text.to_numpy())\n",
        "np.savez_compressed(\"/content/drive/MyDrive/NLP/Datasets/goemotion_val_text_processed\", val.text.to_numpy())\n",
        "np.savez_compressed(\"/content/drive/MyDrive/NLP/Datasets/goemotion_test_labels\", test.labels.to_numpy())\n",
        "np.savez_compressed(\"/content/drive/MyDrive/NLP/Datasets/goemotion_train_labels\", train.labels.to_numpy())\n",
        "np.savez_compressed(\"/content/drive/MyDrive/NLP/Datasets/goemotion_val_labels\", val.labels.to_numpy())"
      ],
      "metadata": {
        "id": "311nVZRRHqek"
      },
      "execution_count": 59,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}